{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+yArl0FHhTqSmKnCE4K+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinnetValllejo/Accidentalidad_Vial_Antioquia/blob/main/Accidentalidad_Vial_Antioquia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVlsdtmbHZi8",
        "outputId": "5a117cbd-9692-42da-c7a1-71ea790d81a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEntorno configurado. Dependencias instaladas y threads limitados.\n"
          ]
        }
      ],
      "source": [
        "# CELDA 1: instalar dependencias y limitar hilos\n",
        "!pip install -q sqlalchemy matplotlib seaborn scikit-learn joblib\n",
        "# CELDA A: instalar dependencias para Streamlit + ngrok (Colab)\n",
        "!pip install -q streamlit plotly sqlalchemy streamlit-aggrid pyngrok\n",
        "\n",
        "# CELDA C: exponer la app con pyngrok y ejecutar streamlit\n",
        "import subprocess, time, os, signal\n",
        "import os, warnings, matplotlib\n",
        "from pyngrok import ngrok\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Control de hilos (para evitar sobrecarga en Colab)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
        "\n",
        "# Forzar backend no interactivo\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "print(\"Entorno configurado. Dependencias instaladas y threads limitados.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 2: importaciones y rutas\n",
        "import re, warnings, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
        "\n",
        "# Rutas y constantes (ajustadas a Colab)\n",
        "CSV_PATH = \"https://raw.githubusercontent.com/ShinnetValllejo/Accidentalidad_Vial_Antioquia/main/AMVA_Accidentalidad_20191022_2.csv\"\n",
        "SEPARATOR = \";\"\n",
        "ENCODING = \"latin-1\"\n",
        "\n",
        "DB_PATH = Path(\"/content/Proyecto_Accidentalidad_Vial_Antioquia.db\")\n",
        "TABLE_NAME = \"Accidentalidad_Vial_Antioquia\"\n",
        "OUT_DIR = Path(\"/content/Graficas_Salida\")\n",
        "MODEL_DIR = Path(\"/content/Modelo_Predict\")\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PALETTE_GREEN = [\"#267344\", \"#37A85B\", \"#A9E4B4\"]\n",
        "print(\"Rutas y entorno listos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2pX4m6HHm8y",
        "outputId": "750ee8d9-f7e6-46fa-fc4e-17e642658d17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rutas y entorno listos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 3: funciones de limpieza (id√©nticas a tu script)\n",
        "def clean_fecha(fecha):\n",
        "    if pd.isna(fecha):\n",
        "        return None\n",
        "    s = str(fecha).strip()\n",
        "    match = re.search(r\"\\d{1,2}/\\d{1,2}/\\d{2,4}\", s)\n",
        "    if not match:\n",
        "        return None\n",
        "    for fmt in [\"%d/%m/%Y\", \"%m/%d/%Y\"]:\n",
        "        try:\n",
        "            return pd.to_datetime(match.group(0), format=fmt).strftime(\"%d/%m/%Y\")\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def clean_hora(hora):\n",
        "    if pd.isna(hora):\n",
        "        return None\n",
        "    s = re.sub(r\"\\s+\", \" \", str(hora).strip().replace(\"\\u00A0\", \" \"))\n",
        "    m = re.search(r\"(\\d{1,2}:\\d{2}(:\\d{2})?)\", s)\n",
        "    s = m.group(1) if m else s\n",
        "    s = s.replace(\"p m\", \"PM\").replace(\"pm\", \"PM\").replace(\"a m\", \"AM\").replace(\"am\", \"AM\")\n",
        "    return s.strip()\n",
        "\n",
        "def try_parse_time(val):\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    for fmt in [\"%I:%M:%S %p\", \"%I:%M %p\", \"%H:%M:%S\", \"%H:%M\"]:\n",
        "        t = pd.to_datetime(val, format=fmt, errors=\"coerce\")\n",
        "        if pd.notna(t):\n",
        "            return t\n",
        "    # fallback: try pandas generic parse\n",
        "    t = pd.to_datetime(val, errors=\"coerce\", dayfirst=True)\n",
        "    return t if pd.notna(t) else None\n",
        "\n",
        "def clasificar_jornada(hora_str):\n",
        "    if pd.isna(hora_str):\n",
        "        return None\n",
        "    try:\n",
        "        h = int(hora_str.split(\":\")[0])\n",
        "        if 0 <= h < 6: return \"MADRUGADA\"\n",
        "        if 6 <= h < 12: return \"MA√ëANA\"\n",
        "        if 12 <= h < 18: return \"TARDE\"\n",
        "        if 18 <= h < 24: return \"NOCHE\"\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "IfknTLXfHv5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 4: carga y limpieza completa (reproduce tu CargaLimpiezaBD.py)\n",
        "print(\"Cargando CSV desde GitHub...\")\n",
        "df = pd.read_csv(CSV_PATH, sep=SEPARATOR, encoding=ENCODING, low_memory=False)\n",
        "print(f\"Datos cargados: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "\n",
        "# Normalizar columnas\n",
        "df.columns = df.columns.str.strip()\n",
        "rename_map = {\n",
        "    \"GRAVEDA√ëOSSADA√ëOSS\": \"GRAVEDAD_ACCIDENTE\",\n",
        "    \"D√çA DE LA SEMANA\": \"NOM_DIA_SEMANA\",\n",
        "    \"DIA DE LA SEMANA\": \"NOM_DIA_SEMANA\"\n",
        "}\n",
        "df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Aplicar limpieza a FECHA y HORA\n",
        "df[\"FECHA\"] = df[\"FECHA\"].astype(str).map(clean_fecha)\n",
        "df[\"HORA\"] = df[\"HORA\"].astype(str).map(clean_hora)\n",
        "\n",
        "# Crear HORA_dt y NUM_HORA\n",
        "df[\"HORA_dt\"] = df[\"HORA\"].apply(try_parse_time)\n",
        "df[\"NUM_HORA\"] = df[\"HORA_dt\"].apply(\n",
        "    lambda t: (t.hour + t.minute / 60.0 + t.second / 3600.0) if pd.notna(t) else None\n",
        ")\n",
        "df[\"HORA\"] = df[\"HORA_dt\"].dt.strftime(\"%H:%M:%S\")\n",
        "df.drop(columns=[\"HORA_dt\"], inplace=True)\n",
        "\n",
        "# Clasificar jornada\n",
        "df[\"JORNADA\"] = df[\"HORA\"].map(clasificar_jornada)\n",
        "\n",
        "# FECHA_dt -> NUM_DIA_SEMANA y NUM_MES\n",
        "df[\"FECHA_dt\"] = pd.to_datetime(df[\"FECHA\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "df[\"NUM_DIA_SEMANA\"] = df[\"FECHA_dt\"].dt.weekday + 1\n",
        "df[\"NUM_MES\"] = df[\"FECHA_dt\"].dt.month\n",
        "df.drop(columns=[\"FECHA_dt\"], inplace=True)\n",
        "\n",
        "# Normalizaci√≥n de textos\n",
        "for col in df.columns:\n",
        "    if col in (\"NUM_HORA\", \"NUM_DIA_SEMANA\", \"NUM_MES\"):\n",
        "        continue\n",
        "    if df[col].dtype == \"object\":\n",
        "        df[col] = (\n",
        "            df[col].astype(str)\n",
        "                   .str.strip()\n",
        "                   .str.upper()\n",
        "                   .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        )\n",
        "\n",
        "# Validaciones finales\n",
        "df[\"FECHA\"] = df[\"FECHA\"].replace(\"NAN\", None)\n",
        "df[\"HORA\"] = df[\"HORA\"].replace(\"NAN\", None)\n",
        "df[\"NUM_HORA\"] = pd.to_numeric(df[\"NUM_HORA\"], errors=\"coerce\")\n",
        "\n",
        "# Guardar en SQLite\n",
        "engine = create_engine(f\"sqlite:///{DB_PATH}\")\n",
        "df.to_sql(TABLE_NAME, con=engine, if_exists=\"replace\", index=False)\n",
        "print(\"Guardado en SQLite:\", DB_PATH)\n",
        "\n",
        "# Reporte de nulos por columna (imprime top)\n",
        "print(\"\\n=== VALIDACI√ìN DE NULOS POR CAMPO (muestra top 20) ===\")\n",
        "nulos_por_columna = df.isna().sum().sort_values(ascending=False)\n",
        "total = len(df)\n",
        "for col, nulos in nulos_por_columna.head(20).items():\n",
        "    pct = (nulos / total) * 100\n",
        "    print(f\"{col:<30} -> {nulos:>6} nulos ({pct:5.2f}%)\")\n",
        "print(\"======================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBTIUyHNHxHJ",
        "outputId": "c4faa528-1044-4c8d-d6e0-b646be3aeeab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando CSV desde GitHub...\n",
            "Datos cargados: 203435 filas x 11 columnas\n",
            "Guardado en SQLite: /content/Proyecto_Accidentalidad_Vial_Antioquia.db\n",
            "\n",
            "=== VALIDACI√ìN DE NULOS POR CAMPO (muestra top 20) ===\n",
            "HORA                           ->      1 nulos ( 0.00%)\n",
            "NUM_HORA                       ->      1 nulos ( 0.00%)\n",
            "MUNICIPIO                      ->      0 nulos ( 0.00%)\n",
            "FECHA                          ->      0 nulos ( 0.00%)\n",
            "NOM_DIA_SEMANA                 ->      0 nulos ( 0.00%)\n",
            "CLASE                          ->      0 nulos ( 0.00%)\n",
            "COD_MUNICIPIO                  ->      0 nulos ( 0.00%)\n",
            "DIRECCI√ìN                      ->      0 nulos ( 0.00%)\n",
            "GRAVEDAD_ACCIDENTE             ->      0 nulos ( 0.00%)\n",
            "COMUNA                         ->      0 nulos ( 0.00%)\n",
            "BARRIO                         ->      0 nulos ( 0.00%)\n",
            "DISE√ëO                         ->      0 nulos ( 0.00%)\n",
            "JORNADA                        ->      0 nulos ( 0.00%)\n",
            "NUM_DIA_SEMANA                 ->      0 nulos ( 0.00%)\n",
            "NUM_MES                        ->      0 nulos ( 0.00%)\n",
            "======================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 5: utilidades para gr√°ficas y carga desde DB\n",
        "def paleta_antioquia(n: int):\n",
        "    if n <= 0: return []\n",
        "    return PALETTE_GREEN[:n] if n <= len(PALETTE_GREEN) else sns.blend_palette(PALETTE_GREEN, n_colors=n)\n",
        "\n",
        "def save_fig(fig, path: Path):\n",
        "    fig.savefig(path, format=\"jpg\", bbox_inches=\"tight\", dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "def num_fmt(v): return f\"{int(v):,}\"\n",
        "\n",
        "def txt_color(rgb):\n",
        "    return \"black\" if (0.299*rgb[0] + 0.587*rgb[1] + 0.114*rgb[2]) > 0.65 else \"white\"\n",
        "\n",
        "def load_table(db_path: Path, table: str) -> pd.DataFrame:\n",
        "    if not db_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe la base de datos: {db_path}\")\n",
        "    with create_engine(f\"sqlite:///{db_path}\").connect() as conn:\n",
        "        return pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
        "\n",
        "def format_torta(series: pd.Series, title: str, path: Path):\n",
        "    colors = paleta_antioquia(len(series)) or paleta_antioquia(1)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.pie(series.values, labels=series.index, autopct=\"%1.1f%%\", startangle=60,\n",
        "           colors=colors[::-1], textprops={\"fontsize\": 12})\n",
        "    ax.set_title(title, fontsize=18, fontweight=\"bold\")\n",
        "    save_fig(fig, path)\n",
        "\n",
        "def format_barra(series: pd.Series, title: str, xlabel: str, ylabel: str, path: Path):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    series = series.dropna()\n",
        "    if series.empty:\n",
        "        ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
        "        save_fig(fig, path)\n",
        "        return\n",
        "    values = series.values.astype(float)\n",
        "    palette = paleta_antioquia(len(series))\n",
        "    total, max_w = values.sum() or 1, values.max() or 1\n",
        "    sns.barplot(x=values, y=series.index, palette=palette, ax=ax)\n",
        "    for p, val in zip(ax.patches, values):\n",
        "        w, y = p.get_width(), p.get_y() + p.get_height()/2\n",
        "        pct, rel = (val/total)*100, w/max_w\n",
        "        color = txt_color(p.get_facecolor()[:3]) if rel >= 0.12 else \"black\"\n",
        "        ax.text(w*0.5 if rel >= 0.12 else w+(max_w*0.01), y,\n",
        "                f\"{num_fmt(val)} ({pct:.1f}%)\",\n",
        "                ha=\"center\" if rel >= 0.12 else \"left\", va=\"center\",\n",
        "                fontweight=\"bold\", fontsize=11, color=color)\n",
        "    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.6)\n",
        "    fig.tight_layout()\n",
        "    save_fig(fig, path)"
      ],
      "metadata": {
        "id": "o8ZRDFzbH2aY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 6: analisis_rapido (gr√°ficas principales)\n",
        "def analisis_rapido(df: pd.DataFrame):\n",
        "    print(\"\\nüìÑ AN√ÅLISIS EXPLORATORIO R√ÅPIDO\")\n",
        "    required = [\"GRAVEDAD_ACCIDENTE\", \"JORNADA\", \"CLASE\", \"COMUNA\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing: raise KeyError(f\"Faltan columnas: {missing}\")\n",
        "\n",
        "    format_torta(df[\"GRAVEDAD_ACCIDENTE\"].value_counts(),\n",
        "                 \"Distribuci√≥n por Gravedad de Accidentes\",\n",
        "                 OUT_DIR / \"Accidentes_Gravedad_SVA.jpg\")\n",
        "\n",
        "    format_barra(df[\"JORNADA\"].value_counts(),\n",
        "                 \"Cantidad de Accidentes por Jornada\",\n",
        "                 \"N√∫mero de Accidentes\", \"Franja Horaria\",\n",
        "                 OUT_DIR / \"Accidentes_Jornada_SVA.jpg\")\n",
        "\n",
        "    df_clase = df[df[\"CLASE\"].str.upper() != \"SIN INFORMACI√ìN\"]\n",
        "    format_barra(df_clase[\"CLASE\"].value_counts().head(10),\n",
        "                 \"Cantidad de Accidentes por Clase\",\n",
        "                 \"N√∫mero de Accidentes\", \"Tipo de Accidente\",\n",
        "                 OUT_DIR / \"Accidentes_Clase_SVA.jpg\")\n",
        "\n",
        "    df_comuna = df[df[\"COMUNA\"].str.upper() != \"SIN INFORMACI√ìN\"]\n",
        "    format_barra(df_comuna[\"COMUNA\"].value_counts().head(10),\n",
        "                 \"Top 10 - Accidentes por Comuna\",\n",
        "                 \"N√∫mero de Accidentes\", \"Comuna\",\n",
        "                 OUT_DIR / \"Accidentes_Comuna_SVA.jpg\")\n",
        "\n",
        "    print(f\"‚úîÔ∏è  Gr√°ficas generadas en: {OUT_DIR}\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "v3GdTGq_H3Xj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 7: preparar_datos (preprocesamiento id√©ntico)\n",
        "def preparar_datos(data: pd.DataFrame):\n",
        "    print(\"üìÑ PREPARANDO DATOS PARA RANDOM FOREST\")\n",
        "    df_local = data.copy()\n",
        "    df_local[\"HERIDOS_MUERTOS\"] = df_local[\"GRAVEDAD_ACCIDENTE\"].str.upper().isin([\"HERIDOS\", \"MUERTOS\"]).astype(np.uint8)\n",
        "    df_local[\"FIN_DE_SEMANA\"] = df_local[\"NUM_DIA_SEMANA\"].isin([6,7]).astype(np.uint8)\n",
        "\n",
        "    numeric_features = ['NUM_MES', 'NUM_DIA_SEMANA', 'NUM_HORA', 'FIN_DE_SEMANA']\n",
        "    categorical_features = ['CLASE', 'MUNICIPIO', 'COMUNA', 'JORNADA']\n",
        "\n",
        "    # Asegurar existencia de columnas categ√≥ricas si faltan, crear con 'SIN INFORMACI√ìN'\n",
        "    for c in categorical_features:\n",
        "        if c not in df_local.columns:\n",
        "            df_local[c] = 'SIN INFORMACI√ìN'\n",
        "\n",
        "    X = df_local[numeric_features + categorical_features]\n",
        "    y = df_local['HERIDOS_MUERTOS']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y )\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_features)\n",
        "    ])\n",
        "\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    print(f\"‚úîÔ∏è  Forma despu√©s del preprocesamiento: {X_train_preprocessed.shape}\")\n",
        "    return X_train, X_test, y_train, y_test, preprocessor"
      ],
      "metadata": {
        "id": "SUJ0EmjAH7pq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 8: entrenamiento, evaluaci√≥n y utilidades del modelo\n",
        "def entrenar_random_forest(X_train, X_test, y_train, y_test, preprocessor):\n",
        "    print(\"\\nü§ñ ENTRENAMIENTO RANDOM FOREST\")\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=5,      # Solo 5 √°rboles como solicitado\n",
        "            random_state=42,\n",
        "            max_depth=5          # Limitamos profundidad para evitar overfitting\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"Exactitud: {acc:.4f} | Precisi√≥n: {prec:.4f} | Sensibilidad: {rec:.4f} | F1: {f1:.4f} | AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=sns.blend_palette(PALETTE_GREEN[::-1], as_cmap=True),\n",
        "                xticklabels=['Solo Da√±os', 'Con Heridos'], yticklabels=['Solo Da√±os', 'Con Heridos'], ax=ax)\n",
        "    ax.set_title('Matriz de Confusi√≥n - Clasificaci√≥n de Accidentes', fontsize=16)\n",
        "    save_fig(fig, OUT_DIR / \"Matriz_Confusion_SVA.jpg\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.plot(fpr, tpr, color=PALETTE_GREEN[1], lw=2, label=f'ROC (AUC={roc_auc:.2f})')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\n",
        "    ax.set_xlim([-0.00, 1.01]); ax.set_ylim([-0.00, 1.01])\n",
        "    ax.set_title('Curva ROC - Clasificaci√≥n de Severidad de Accidentes')\n",
        "    ax.set_xlabel('Tasa Falsos Positivos'); ax.set_ylabel('Tasa Verdaderos Positivos')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    save_fig(fig, OUT_DIR / \"Curva_ROC_SVA.jpg\")\n",
        "\n",
        "    print(classification_report(y_test, y_pred, target_names=['Solo Da√±os', 'Con Heridos']))\n",
        "    return model, {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': roc_auc}\n",
        "\n",
        "def guardar_modelo(modelo):\n",
        "    path = MODEL_DIR / \"Modelo_RandomForest_SVA.joblib\"\n",
        "    joblib.dump(modelo, path)\n",
        "    print(f\"‚úîÔ∏è Modelo guardado en: {path}\")\n",
        "\n",
        "def hacer_predicciones(modelo):\n",
        "    nuevos = pd.DataFrame({\n",
        "        'NUM_MES': [1,7,12,3],\n",
        "        'NUM_DIA_SEMANA': [6,2,4,1],\n",
        "        'NUM_HORA': [20,14,8,18],\n",
        "        'FIN_DE_SEMANA': [1,0,0,0],\n",
        "        'CLASE': ['CHOQUE','ATROPELLO','CHOQUE','VOLCAMIENTO'],\n",
        "        'MUNICIPIO': ['MEDELL√çN']*4,\n",
        "        'COMUNA': ['LAURELES ESTADIO','LA CANDELARIA','CASTILLA','ROBLEDO'],\n",
        "        'JORNADA': ['NOCHE','TARDE','MA√ëANA','TARDE']\n",
        "    })\n",
        "    preds = modelo.predict(nuevos)\n",
        "    proba = modelo.predict_proba(nuevos)[:, 1]\n",
        "    resultados = nuevos.copy()\n",
        "    resultados['PREDICCION'] = np.where(preds==1,'CON HERIDOS','SOLO DA√ëOS')\n",
        "    resultados['PROBABILIDAD_HERIDOS'] = [f\"{p:.1%}\" for p in proba]\n",
        "    resultados['RIESGO'] = np.select([proba>0.7, proba>0.5], ['ALTO','MEDIO'], default='BAJO')\n",
        "    save_path = MODEL_DIR / \"Predicciones_Nuevos_Accidentes.csv\"\n",
        "    resultados.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úîÔ∏è Predicciones guardadas en {save_path}\")\n",
        "    return resultados\n",
        "\n",
        "def analizar_importancia_variables(modelo: Pipeline, preprocessor: ColumnTransformer):\n",
        "    rf = modelo.named_steps['classifier']\n",
        "    num = ['NUM_MES','NUM_DIA_SEMANA','NUM_HORA','FIN_DE_SEMANA']\n",
        "    cat = ['CLASE','MUNICIPIO','COMUNA','JORNADA']\n",
        "    ohe = preprocessor.named_transformers_['cat']\n",
        "    try:\n",
        "        cat_names = ohe.get_feature_names_out(cat)\n",
        "    except:\n",
        "        # compatibilidad sklearn antigua\n",
        "        cat_names = ohe.get_feature_names(cat)\n",
        "    names = num + list(cat_names)\n",
        "    imp = rf.feature_importances_\n",
        "    imp_df = pd.DataFrame({'Variable': names, 'Importancia': imp}).sort_values('Importancia', ascending=False)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    sns.barplot(x='Importancia', y='Variable', data=imp_df.head(10), palette=paleta_antioquia(10), ax=ax)\n",
        "    ax.set_title('Top 10 Variables M√°s Importantes - Random Forest', fontsize=16)\n",
        "    save_fig(fig, MODEL_DIR / \"Importancia_Variables_RF.jpg\")\n",
        "    imp_df.to_csv(MODEL_DIR / \"Importancia_Variables_RF.csv\", index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úîÔ∏è Importancia de variables guardada.\")\n",
        "    return imp_df"
      ],
      "metadata": {
        "id": "zMe4uzJoH8n8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 9: resumen ejecutivo y ejecuci√≥n final controlada\n",
        "def generar_resumen_final(df: pd.DataFrame, resultados: dict):\n",
        "    resumen_path = MODEL_DIR / \"Resumen_Ejecutivo_Modelo.txt\"\n",
        "    with open(resumen_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"üìÑ RESUMEN EJECUTIVO DEL PROYECTO\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        total_accidentes = len(df)\n",
        "        accidentes_con_heridos = len(df[df['GRAVEDAD_ACCIDENTE'].str.upper().isin(['HERIDOS', 'MUERTOS'])])\n",
        "        tasa_heridos = accidentes_con_heridos / total_accidentes if total_accidentes else 0\n",
        "\n",
        "        f.write(\"üìà ESTAD√çSTICAS GENERALES:\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Total de accidentes analizados: {total_accidentes:,}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Accidentes con heridos/muertos: {accidentes_con_heridos:,}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Tasa de accidentes con heridos: {tasa_heridos:.2%}\\n\\n\")\n",
        "\n",
        "        f.write(\"ü§ñ RESULTADOS DEL MODELO RANDOM FOREST:\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Exactitud: {resultados['accuracy']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Precisi√≥n: {resultados['precision']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Sensibilidad: {resultados['recall']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ F1-Score: {resultados['f1']:.4f}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ AUC-ROC: {resultados['roc_auc']:.4f}\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            franja_peligrosa = (\n",
        "                df.groupby('JORNADA')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "            tipo_peligroso = (\n",
        "                df.groupby('CLASE')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "            comuna_peligrosa = (\n",
        "                df.groupby('COMUNA')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "\n",
        "            f.write(\"üîç HALLAZGOS PRINCIPALES:\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Franja horaria m√°s peligrosa: {franja_peligrosa}\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Tipo de accidente m√°s peligroso: {tipo_peligroso}\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Comuna con mayor tasa de heridos: {comuna_peligrosa}\\n\\n\")\n",
        "\n",
        "            f.write(\"üí° RECOMENDACIONES:\\n\")\n",
        "            f.write(f\"   1. Reforzar vigilancia en: {franja_peligrosa}\\n\")\n",
        "            f.write(f\"   2. Implementar campa√±as preventivas para: {tipo_peligroso}\\n\")\n",
        "            f.write(f\"   3. Focalizar recursos de control en: {comuna_peligrosa}\\n\")\n",
        "            f.write(\"   4. Utilizar el modelo predictivo para priorizar zonas de riesgo.\\n\")\n",
        "        except Exception as e:\n",
        "            f.write(f\"‚ùå No se pudieron generar hallazgos detallados: {e}\\n\")\n",
        "\n",
        "    print(f\"‚úîÔ∏è Resumen ejecutivo guardado en: {resumen_path}\")\n",
        "\n",
        "# EJECUCI√ìN CONTROLADA (main)\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df_db = load_table(DB_PATH, TABLE_NAME)\n",
        "        analisis_rapido(df_db)\n",
        "        X_train, X_test, y_train, y_test, preprocessor = preparar_datos(df_db)\n",
        "        modelo_rf, resultados = entrenar_random_forest(X_train, X_test, y_train, y_test, preprocessor)\n",
        "        preds_df = hacer_predicciones(modelo_rf)\n",
        "        guardar_modelo(modelo_rf)\n",
        "        imp_df = analizar_importancia_variables(modelo_rf, preprocessor)\n",
        "        generar_resumen_final(df_db, resultados)\n",
        "        print(\"\\n‚úîÔ∏è Flujo completo: ejecutado correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la ejecuci√≥n: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMfkTi8GIAA6",
        "outputId": "8bca5bcd-ec1d-4327-87f2-ff0ebb648621"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ AN√ÅLISIS EXPLORATORIO R√ÅPIDO\n",
            "‚úîÔ∏è  Gr√°ficas generadas en: /content/Graficas_Salida\n",
            "============================================================\n",
            "üìÑ PREPARANDO DATOS PARA RANDOM FOREST\n",
            "‚úîÔ∏è  Forma despu√©s del preprocesamiento: (162748, 47)\n",
            "\n",
            "ü§ñ ENTRENAMIENTO RANDOM FOREST\n",
            "Exactitud: 0.7662 | Precisi√≥n: 0.9729 | Sensibilidad: 0.5513 | F1: 0.7038 | AUC-ROC: 0.8033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Solo Da√±os       0.68      0.98      0.81     20187\n",
            " Con Heridos       0.97      0.55      0.70     20500\n",
            "\n",
            "    accuracy                           0.77     40687\n",
            "   macro avg       0.83      0.77      0.76     40687\n",
            "weighted avg       0.83      0.77      0.75     40687\n",
            "\n",
            "‚úîÔ∏è Predicciones guardadas en /content/Modelo_Predict/Predicciones_Nuevos_Accidentes.csv\n",
            "‚úîÔ∏è Modelo guardado en: /content/Modelo_Predict/Modelo_RandomForest_SVA.joblib\n",
            "‚úîÔ∏è Importancia de variables guardada.\n",
            "‚úîÔ∏è Resumen ejecutivo guardado en: /content/Modelo_Predict/Resumen_Ejecutivo_Modelo.txt\n",
            "\n",
            "‚úîÔ∏è Flujo completo: ejecutado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 10: listar archivos clave generados\n",
        "import glob\n",
        "print(\"\\nArchivos generados (jpg, db, csv, joblib, txt):\")\n",
        "for file in sorted(glob.glob(\"/content/**/*\", recursive=True)):\n",
        "    if any(file.lower().endswith(ext) for ext in [\".jpg\", \".db\", \".csv\", \".joblib\", \".txt\"]):\n",
        "        print(\" \", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4YqIrZFIDII",
        "outputId": "975ff112-92e9-4fa7-e8bc-eb7225305c0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivos generados (jpg, db, csv, joblib, txt):\n",
            "  /content/sample_data/california_housing_test.csv\n",
            "  /content/sample_data/california_housing_train.csv\n",
            "  /content/sample_data/mnist_test.csv\n",
            "  /content/sample_data/mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA B: crear streamlit_app.py\n",
        "code = r'''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sqlalchemy import create_engine\n",
        "import joblib\n",
        "\n",
        "# Config\n",
        "PALETTE_GREEN = [\"#267344\", \"#37A85B\", \"#A9E4B4\"]\n",
        "DB_PATH = \"/content/Proyecto_Accidentalidad_Vial_Antioquia.db\"\n",
        "MODEL_DIR = Path(\"/content/Modelo_Predict\")\n",
        "OUT_DIR = Path(\"/content/Graficas_Salida\")\n",
        "\n",
        "@st.cache_data\n",
        "def load_db(table_name=\"Accidentalidad_Vial_Antioquia\"):\n",
        "    engine = create_engine(f\"sqlite:///{DB_PATH}\")\n",
        "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
        "    return df\n",
        "\n",
        "@st.cache_data\n",
        "def load_model_metrics():\n",
        "    rpt = MODEL_DIR / \"reporte_metricas.csv\"\n",
        "    if rpt.exists():\n",
        "        try:\n",
        "            return pd.read_csv(rpt, index_col=0)\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "@st.cache_data\n",
        "def load_importance():\n",
        "    p = MODEL_DIR / \"Importancia_Variables_RF.csv\"\n",
        "    if p.exists():\n",
        "        return pd.read_csv(p)\n",
        "    return None\n",
        "\n",
        "@st.cache_data\n",
        "def load_model():\n",
        "    p = MODEL_DIR / \"Modelo_RandomForest_SVA.joblib\"\n",
        "    if p.exists():\n",
        "        return joblib.load(p)\n",
        "    return None\n",
        "\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Accidentalidad Vial - Antioquia\", initial_sidebar_state=\"expanded\")\n",
        "\n",
        "st.sidebar.title(\"Filtros\")\n",
        "df = load_db()\n",
        "\n",
        "# Sidebar filters\n",
        "min_mes = int(df[\"NUM_MES\"].min()) if \"NUM_MES\" in df.columns else 1\n",
        "max_mes = int(df[\"NUM_MES\"].max()) if \"NUM_MES\" in df.columns else 12\n",
        "mes_range = st.sidebar.slider(\"Mes (rango)\", min_mes, max_mes, (min_mes, max_mes))\n",
        "\n",
        "comunas = sorted(df[\"COMUNA\"].dropna().unique()) if \"COMUNA\" in df.columns else []\n",
        "comuna_sel = st.sidebar.multiselect(\"Comuna\", options=comunas, default=comunas[:5])\n",
        "\n",
        "clases = sorted(df[\"CLASE\"].dropna().unique()) if \"CLASE\" in df.columns else []\n",
        "clase_sel = st.sidebar.multiselect(\"Clase de accidente\", options=clases, default=clases[:5])\n",
        "\n",
        "jornadas = sorted(df[\"JORNADA\"].dropna().unique()) if \"JORNADA\" in df.columns else []\n",
        "jornada_sel = st.sidebar.multiselect(\"Jornada\", options=jornadas, default=jornadas)\n",
        "\n",
        "# Apply filters\n",
        "df_f = df.copy()\n",
        "if \"NUM_MES\" in df_f.columns:\n",
        "    df_f = df_f[df_f[\"NUM_MES\"].between(mes_range[0], mes_range[1])]\n",
        "if comuna_sel:\n",
        "    df_f = df_f[df_f[\"COMUNA\"].isin(comuna_sel)]\n",
        "if clase_sel:\n",
        "    df_f = df_f[df_f[\"CLASE\"].isin(clase_sel)]\n",
        "if jornada_sel:\n",
        "    df_f = df_f[df_f[\"JORNADA\"].isin(jornada_sel)]\n",
        "\n",
        "# Top KPIs\n",
        "st.title(\"Accidentalidad Vial - Antioquia\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "total = len(df_f)\n",
        "heridos = df_f[\"GRAVEDAD_ACCIDENTE\"].str.upper().isin([\"HERIDOS\", \"MUERTOS\"]).sum() if \"GRAVEDAD_ACCIDENTE\" in df_f.columns else 0\n",
        "tasa = (heridos/total) if total else 0\n",
        "\n",
        "col1.metric(\"Total registros\", f\"{total:,}\")\n",
        "col2.metric(\"Accidentes con heridos/muertos\", f\"{heridos:,}\")\n",
        "col3.metric(\"Tasa heridos\", f\"{tasa:.2%}\")\n",
        "\n",
        "# A: Distribuci√≥n de gravedad (pie + barras por mes)\n",
        "st.header(\"Distribuci√≥n por gravedad\")\n",
        "if \"GRAVEDAD_ACCIDENTE\" in df_f.columns:\n",
        "    dist = df_f[\"GRAVEDAD_ACCIDENTE\"].value_counts().reset_index()\n",
        "    dist.columns = [\"GRAVEDAD\", \"COUNT\"]\n",
        "    fig_pie = px.pie(dist, values=\"COUNT\", names=\"GRAVEDAD\", color=\"GRAVEDAD\",\n",
        "                     color_discrete_sequence=PALETTE_GREEN)\n",
        "    st.plotly_chart(fig_pie, use_container_width=True)\n",
        "\n",
        "    # barras por mes\n",
        "    if \"NUM_MES\" in df_f.columns:\n",
        "        by_month = df_f.groupby([\"NUM_MES\", \"GRAVEDAD_ACCIDENTE\"]).size().reset_index(name=\"COUNT\")\n",
        "        fig_bar = px.bar(by_month, x=\"NUM_MES\", y=\"COUNT\", color=\"GRAVEDAD_ACCIDENTE\",\n",
        "                         color_discrete_sequence=PALETTE_GREEN, barmode=\"stack\",\n",
        "                         labels={\"NUM_MES\": \"Mes\"})\n",
        "        st.plotly_chart(fig_bar, use_container_width=True)\n",
        "\n",
        "# B: Jornada (barra + heatmap horas vs d√≠a)\n",
        "st.header(\"Accidentes por jornada y hora\")\n",
        "if \"JORNADA\" in df_f.columns:\n",
        "    jcounts = df_f[\"JORNADA\"].value_counts().reset_index()\n",
        "    jcounts.columns = [\"JORNADA\", \"COUNT\"]\n",
        "    fig_j = px.bar(jcounts, x=\"COUNT\", y=\"JORNADA\", orientation=\"h\", color=\"JORNADA\",\n",
        "                   color_discrete_sequence=PALETTE_GREEN)\n",
        "    st.plotly_chart(fig_j, use_container_width=True)\n",
        "\n",
        "if \"NUM_DIA_SEMANA\" in df_f.columns and \"NUM_HORA\" in df_f.columns:\n",
        "    pivot = df_f.dropna(subset=[\"NUM_DIA_SEMANA\",\"NUM_HORA\"]).groupby([\"NUM_DIA_SEMANA\",\"NUM_HORA\"]).size().reset_index(name=\"COUNT\")\n",
        "    pivot = pivot.pivot(index=\"NUM_DIA_SEMANA\", columns=\"NUM_HORA\", values=\"COUNT\").fillna(0)\n",
        "    fig_heat = go.Figure(data=go.Heatmap(z=pivot.values, x=pivot.columns, y=pivot.index, colorscale=\"Greens\"))\n",
        "    fig_heat.update_layout(xaxis_title=\"Hora (decimal)\", yaxis_title=\"D√≠a de la semana\")\n",
        "    st.plotly_chart(fig_heat, use_container_width=True)\n",
        "\n",
        "# C: Top clases y comunas\n",
        "st.header(\"Top clases y comunas\")\n",
        "if \"CLASE\" in df_f.columns:\n",
        "    top_clase = df_f[\"CLASE\"].value_counts().head(10).reset_index()\n",
        "    top_clase.columns = [\"CLASE\",\"COUNT\"]\n",
        "    fig_cl = px.bar(top_clase, x=\"COUNT\", y=\"CLASE\", orientation=\"h\", color=\"CLASE\", color_discrete_sequence=PALETTE_GREEN)\n",
        "    st.plotly_chart(fig_cl, use_container_width=True)\n",
        "\n",
        "if \"COMUNA\" in df_f.columns:\n",
        "    top_com = df_f[\"COMUNA\"].value_counts().head(10).reset_index()\n",
        "    top_com.columns = [\"COMUNA\",\"COUNT\"]\n",
        "    fig_co = px.bar(top_com, x=\"COUNT\", y=\"COMUNA\", orientation=\"h\", color=\"COMUNA\", color_discrete_sequence=PALETTE_GREEN)\n",
        "    st.plotly_chart(fig_co, use_container_width=True)\n",
        "\n",
        "# Modelado: m√©tricas, matriz, ROC, importancia\n",
        "st.header(\"Evaluaci√≥n del modelo\")\n",
        "model = load_model()\n",
        "metrics = load_model_metrics()\n",
        "if metrics is not None:\n",
        "    st.subheader(\"Reporte de m√©tricas (csv)\")\n",
        "    st.dataframe(metrics)\n",
        "\n",
        "imp = load_importance()\n",
        "if imp is not None:\n",
        "    st.subheader(\"Importancia de variables (top 10)\")\n",
        "    st.dataframe(imp.head(10))\n",
        "    fig_imp = px.bar(imp.sort_values(\"Importancia\", ascending=False).head(10), x=\"Importancia\", y=\"Variable\", orientation=\"h\", color=\"Importancia\", color_continuous_scale=\"Greens\")\n",
        "    st.plotly_chart(fig_imp, use_container_width=True)\n",
        "\n",
        "# Mostrar matriz y ROC desde im√°genes si existen\n",
        "if (OUT_DIR / \"Matriz_Confusion_SVA.jpg\").exists():\n",
        "    st.image(str(OUT_DIR / \"Matriz_Confusion_SVA.jpg\"), caption=\"Matriz de Confusi√≥n\", use_column_width=True)\n",
        "if (OUT_DIR / \"Curva_ROC_SVA.jpg\").exists():\n",
        "    st.image(str(OUT_DIR / \"Curva_ROC_SVA.jpg\"), caption=\"Curva ROC\", use_column_width=True)\n",
        "\n",
        "# Predicciones de ejemplo\n",
        "st.header(\"Predicciones de ejemplo\")\n",
        "preds_path = MODEL_DIR / \"Predicciones_Nuevos_Accidentes.csv\"\n",
        "if preds_path.exists():\n",
        "    preds = pd.read_csv(preds_path)\n",
        "    st.dataframe(preds)\n",
        "    st.download_button(\"Descargar predicciones (CSV)\", data=preds.to_csv(index=False).encode(\"utf-8-sig\"), file_name=\"Predicciones_Nuevos_Accidentes.csv\")\n",
        "\n",
        "# Resumen ejecutivo\n",
        "st.header(\"Resumen ejecutivo\")\n",
        "res_path = MODEL_DIR / \"Resumen_Ejecutivo_Modelo.txt\"\n",
        "if res_path.exists():\n",
        "    with open(res_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        txt = f.read()\n",
        "    st.text_area(\"Resumen ejecutivo\", txt, height=300)\n",
        "    st.download_button(\"Descargar resumen (TXT)\", data=txt.encode(\"utf-8\"), file_name=\"Resumen_Ejecutivo_Modelo.txt\")\n",
        "\n",
        "# Descargas: BD y ZIP resultados\n",
        "st.sidebar.subheader(\"Exportar\")\n",
        "if st.sidebar.button(\"Descargar BD SQLite\"):\n",
        "    with open(DB_PATH, \"rb\") as f:\n",
        "        st.sidebar.download_button(\"Descargar BD\", data=f, file_name=\"Proyecto_Accidentalidad_Vial_Antioquia.db\")\n",
        "'''\n",
        "Path(\"streamlit_app.py\").write_text(code, encoding=\"utf-8\")\n",
        "print(\"Archivo streamlit_app.py creado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8RJld8SloaX",
        "outputId": "955618fe-19f9-48b2-c714-4e74ad5dffac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo streamlit_app.py creado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kill posible proceso previo\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ejecutar streamlit en background\n",
        "port = 8501\n",
        "cmd = f\"streamlit run streamlit_app.py --server.port {port} --server.headless true\"\n",
        "proc = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# abrir tunel\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\"Streamlit corriendo en:\", public_url)\n",
        "print(\"Detener la app con ngrok.kill() o interrumpiendo la celda.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "BA7c7oILluHL",
        "outputId": "4f44c97f-d5f5-4a18-8d88-ec2ceb7ad209"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-11-03T21:46:56+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-11-03T21:46:56+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-11-03T21:46:56+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1517270278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# abrir tunel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streamlit corriendo en:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detener la app con ngrok.kill() o interrumpiendo la celda.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    }
  ]
}