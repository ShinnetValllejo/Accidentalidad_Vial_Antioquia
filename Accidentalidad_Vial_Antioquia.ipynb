{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfBp7+57BfKwjrbhNlgwsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinnetValllejo/Proyecto_Accidentalidad_Vial_Antioquia/blob/main/Accidentalidad_Vial_Antioquia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVlsdtmbHZi8",
        "outputId": "36d4f62c-60d0-407a-8b66-d61c3d4db80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno configurado. Dependencias instaladas y threads limitados.\n"
          ]
        }
      ],
      "source": [
        "# CELDA 1: instalar dependencias y limitar hilos\n",
        "\n",
        "!pip install -q sqlalchemy matplotlib seaborn scikit-learn joblib\n",
        "\n",
        "import os, warnings, matplotlib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Control de hilos (para evitar sobrecarga en Colab)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
        "\n",
        "# Forzar backend no interactivo\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "print(\"Entorno configurado. Dependencias instaladas y threads limitados.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 2: importaciones y rutas\n",
        "import re, warnings, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
        "\n",
        "# Rutas y constantes (ajustadas a Colab)\n",
        "CSV_PATH = \"https://raw.githubusercontent.com/ShinnetValllejo/Proyecto_Accidentalidad_Vial_Antioquia/ac2e86c1d79762dd869003d8207e1ba6b69d5772/AMVA_Accidentalidad_20191022_2.csv\"\n",
        "SEPARATOR = \";\"\n",
        "ENCODING = \"latin-1\"\n",
        "\n",
        "DB_PATH = Path(\"/content/Proyecto_Accidentalidad_Vial_Antioquia.db\")\n",
        "TABLE_NAME = \"Accidentalidad_Vial_Antioquia\"\n",
        "OUT_DIR = Path(\"/content/Graficas_Salida\")\n",
        "MODEL_DIR = Path(\"/content/Modelo_Predict\")\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PALETTE_GREEN = [\"#267344\", \"#37A85B\", \"#A9E4B4\"]\n",
        "print(\"Rutas y entorno listos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2pX4m6HHm8y",
        "outputId": "1bbfd532-88c9-4b05-9d4a-42363bd110c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rutas y entorno listos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 3: funciones de limpieza (id√©nticas a tu script)\n",
        "def clean_fecha(fecha):\n",
        "    if pd.isna(fecha):\n",
        "        return None\n",
        "    s = str(fecha).strip()\n",
        "    match = re.search(r\"\\d{1,2}/\\d{1,2}/\\d{2,4}\", s)\n",
        "    if not match:\n",
        "        return None\n",
        "    for fmt in [\"%d/%m/%Y\", \"%m/%d/%Y\"]:\n",
        "        try:\n",
        "            return pd.to_datetime(match.group(0), format=fmt).strftime(\"%d/%m/%Y\")\n",
        "        except:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def clean_hora(hora):\n",
        "    if pd.isna(hora):\n",
        "        return None\n",
        "    s = re.sub(r\"\\s+\", \" \", str(hora).strip().replace(\"\\u00A0\", \" \"))\n",
        "    m = re.search(r\"(\\d{1,2}:\\d{2}(:\\d{2})?)\", s)\n",
        "    s = m.group(1) if m else s\n",
        "    s = s.replace(\"p m\", \"PM\").replace(\"pm\", \"PM\").replace(\"a m\", \"AM\").replace(\"am\", \"AM\")\n",
        "    return s.strip()\n",
        "\n",
        "def try_parse_time(val):\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    for fmt in [\"%I:%M:%S %p\", \"%I:%M %p\", \"%H:%M:%S\", \"%H:%M\"]:\n",
        "        t = pd.to_datetime(val, format=fmt, errors=\"coerce\")\n",
        "        if pd.notna(t):\n",
        "            return t\n",
        "    # fallback: try pandas generic parse\n",
        "    t = pd.to_datetime(val, errors=\"coerce\", dayfirst=True)\n",
        "    return t if pd.notna(t) else None\n",
        "\n",
        "def clasificar_jornada(hora_str):\n",
        "    if pd.isna(hora_str):\n",
        "        return None\n",
        "    try:\n",
        "        h = int(hora_str.split(\":\")[0])\n",
        "        if 0 <= h < 6: return \"MADRUGADA\"\n",
        "        if 6 <= h < 12: return \"MA√ëANA\"\n",
        "        if 12 <= h < 18: return \"TARDE\"\n",
        "        if 18 <= h < 24: return \"NOCHE\"\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "IfknTLXfHv5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 4: carga y limpieza completa (reproduce tu CargaLimpiezaBD.py)\n",
        "print(\"Cargando CSV desde GitHub...\")\n",
        "df = pd.read_csv(CSV_PATH, sep=SEPARATOR, encoding=ENCODING, low_memory=False)\n",
        "print(f\"Datos cargados: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "\n",
        "# Normalizar columnas\n",
        "df.columns = df.columns.str.strip()\n",
        "rename_map = {\n",
        "    \"GRAVEDA√ëOSSADA√ëOSS\": \"GRAVEDAD_ACCIDENTE\",\n",
        "    \"D√çA DE LA SEMANA\": \"NOM_DIA_SEMANA\",\n",
        "    \"DIA DE LA SEMANA\": \"NOM_DIA_SEMANA\"\n",
        "}\n",
        "df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Aplicar limpieza a FECHA y HORA\n",
        "df[\"FECHA\"] = df[\"FECHA\"].astype(str).map(clean_fecha)\n",
        "df[\"HORA\"] = df[\"HORA\"].astype(str).map(clean_hora)\n",
        "\n",
        "# Crear HORA_dt y NUM_HORA\n",
        "df[\"HORA_dt\"] = df[\"HORA\"].apply(try_parse_time)\n",
        "df[\"NUM_HORA\"] = df[\"HORA_dt\"].apply(\n",
        "    lambda t: (t.hour + t.minute / 60.0 + t.second / 3600.0) if pd.notna(t) else None\n",
        ")\n",
        "df[\"HORA\"] = df[\"HORA_dt\"].dt.strftime(\"%H:%M:%S\")\n",
        "df.drop(columns=[\"HORA_dt\"], inplace=True)\n",
        "\n",
        "# Clasificar jornada\n",
        "df[\"JORNADA\"] = df[\"HORA\"].map(clasificar_jornada)\n",
        "\n",
        "# FECHA_dt -> NUM_DIA_SEMANA y NUM_MES\n",
        "df[\"FECHA_dt\"] = pd.to_datetime(df[\"FECHA\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "df[\"NUM_DIA_SEMANA\"] = df[\"FECHA_dt\"].dt.weekday + 1\n",
        "df[\"NUM_MES\"] = df[\"FECHA_dt\"].dt.month\n",
        "df.drop(columns=[\"FECHA_dt\"], inplace=True)\n",
        "\n",
        "# Normalizaci√≥n de textos\n",
        "for col in df.columns:\n",
        "    if col in (\"NUM_HORA\", \"NUM_DIA_SEMANA\", \"NUM_MES\"):\n",
        "        continue\n",
        "    if df[col].dtype == \"object\":\n",
        "        df[col] = (\n",
        "            df[col].astype(str)\n",
        "                   .str.strip()\n",
        "                   .str.upper()\n",
        "                   .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        )\n",
        "\n",
        "# Validaciones finales\n",
        "df[\"FECHA\"] = df[\"FECHA\"].replace(\"NAN\", None)\n",
        "df[\"HORA\"] = df[\"HORA\"].replace(\"NAN\", None)\n",
        "df[\"NUM_HORA\"] = pd.to_numeric(df[\"NUM_HORA\"], errors=\"coerce\")\n",
        "\n",
        "# Guardar en SQLite\n",
        "engine = create_engine(f\"sqlite:///{DB_PATH}\")\n",
        "df.to_sql(TABLE_NAME, con=engine, if_exists=\"replace\", index=False)\n",
        "print(\"Guardado en SQLite:\", DB_PATH)\n",
        "\n",
        "# Reporte de nulos por columna (imprime top)\n",
        "print(\"\\n=== VALIDACI√ìN DE NULOS POR CAMPO (muestra top 20) ===\")\n",
        "nulos_por_columna = df.isna().sum().sort_values(ascending=False)\n",
        "total = len(df)\n",
        "for col, nulos in nulos_por_columna.head(20).items():\n",
        "    pct = (nulos / total) * 100\n",
        "    print(f\"{col:<30} -> {nulos:>6} nulos ({pct:5.2f}%)\")\n",
        "print(\"======================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBTIUyHNHxHJ",
        "outputId": "a9e315af-9a63-490f-8618-48725827c0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando CSV desde GitHub...\n",
            "Datos cargados: 203435 filas x 11 columnas\n",
            "Guardado en SQLite: /content/Proyecto_Accidentalidad_Vial_Antioquia.db\n",
            "\n",
            "=== VALIDACI√ìN DE NULOS POR CAMPO (muestra top 20) ===\n",
            "HORA                           ->      1 nulos ( 0.00%)\n",
            "NUM_HORA                       ->      1 nulos ( 0.00%)\n",
            "MUNICIPIO                      ->      0 nulos ( 0.00%)\n",
            "FECHA                          ->      0 nulos ( 0.00%)\n",
            "NOM_DIA_SEMANA                 ->      0 nulos ( 0.00%)\n",
            "CLASE                          ->      0 nulos ( 0.00%)\n",
            "COD_MUNICIPIO                  ->      0 nulos ( 0.00%)\n",
            "DIRECCI√ìN                      ->      0 nulos ( 0.00%)\n",
            "GRAVEDAD_ACCIDENTE             ->      0 nulos ( 0.00%)\n",
            "COMUNA                         ->      0 nulos ( 0.00%)\n",
            "BARRIO                         ->      0 nulos ( 0.00%)\n",
            "DISE√ëO                         ->      0 nulos ( 0.00%)\n",
            "JORNADA                        ->      0 nulos ( 0.00%)\n",
            "NUM_DIA_SEMANA                 ->      0 nulos ( 0.00%)\n",
            "NUM_MES                        ->      0 nulos ( 0.00%)\n",
            "======================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 5: utilidades para gr√°ficas y carga desde DB\n",
        "def paleta_antioquia(n: int):\n",
        "    if n <= 0: return []\n",
        "    return PALETTE_GREEN[:n] if n <= len(PALETTE_GREEN) else sns.blend_palette(PALETTE_GREEN, n_colors=n)\n",
        "\n",
        "def save_fig(fig, path: Path):\n",
        "    fig.savefig(path, format=\"jpg\", bbox_inches=\"tight\", dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "def num_fmt(v): return f\"{int(v):,}\"\n",
        "\n",
        "def txt_color(rgb):\n",
        "    return \"black\" if (0.299*rgb[0] + 0.587*rgb[1] + 0.114*rgb[2]) > 0.65 else \"white\"\n",
        "\n",
        "def load_table(db_path: Path, table: str) -> pd.DataFrame:\n",
        "    if not db_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe la base de datos: {db_path}\")\n",
        "    with create_engine(f\"sqlite:///{db_path}\").connect() as conn:\n",
        "        return pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
        "\n",
        "def format_torta(series: pd.Series, title: str, path: Path):\n",
        "    colors = paleta_antioquia(len(series)) or paleta_antioquia(1)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.pie(series.values, labels=series.index, autopct=\"%1.1f%%\", startangle=60,\n",
        "           colors=colors[::-1], textprops={\"fontsize\": 12})\n",
        "    ax.set_title(title, fontsize=18, fontweight=\"bold\")\n",
        "    save_fig(fig, path)\n",
        "\n",
        "def format_barra(series: pd.Series, title: str, xlabel: str, ylabel: str, path: Path):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    series = series.dropna()\n",
        "    if series.empty:\n",
        "        ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
        "        save_fig(fig, path)\n",
        "        return\n",
        "    values = series.values.astype(float)\n",
        "    palette = paleta_antioquia(len(series))\n",
        "    total, max_w = values.sum() or 1, values.max() or 1\n",
        "    sns.barplot(x=values, y=series.index, palette=palette, ax=ax)\n",
        "    for p, val in zip(ax.patches, values):\n",
        "        w, y = p.get_width(), p.get_y() + p.get_height()/2\n",
        "        pct, rel = (val/total)*100, w/max_w\n",
        "        color = txt_color(p.get_facecolor()[:3]) if rel >= 0.12 else \"black\"\n",
        "        ax.text(w*0.5 if rel >= 0.12 else w+(max_w*0.01), y,\n",
        "                f\"{num_fmt(val)} ({pct:.1f}%)\",\n",
        "                ha=\"center\" if rel >= 0.12 else \"left\", va=\"center\",\n",
        "                fontweight=\"bold\", fontsize=11, color=color)\n",
        "    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.7, alpha=0.6)\n",
        "    fig.tight_layout()\n",
        "    save_fig(fig, path)"
      ],
      "metadata": {
        "id": "o8ZRDFzbH2aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 6: analisis_rapido (gr√°ficas principales)\n",
        "def analisis_rapido(df: pd.DataFrame):\n",
        "    print(\"\\nüìÑ AN√ÅLISIS EXPLORATORIO R√ÅPIDO\")\n",
        "    required = [\"GRAVEDAD_ACCIDENTE\", \"JORNADA\", \"CLASE\", \"COMUNA\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing: raise KeyError(f\"Faltan columnas: {missing}\")\n",
        "\n",
        "    format_torta(df[\"GRAVEDAD_ACCIDENTE\"].value_counts(),\n",
        "                 \"Distribuci√≥n por Gravedad de Accidentes\",\n",
        "                 OUT_DIR / \"Accidentes_Gravedad_SVA.jpg\")\n",
        "\n",
        "    format_barra(df[\"JORNADA\"].value_counts(),\n",
        "                 \"Cantidad de Accidentes por Jornada\",\n",
        "                 \"N√∫mero de Accidentes\", \"Franja Horaria\",\n",
        "                 OUT_DIR / \"Accidentes_Jornada_SVA.jpg\")\n",
        "\n",
        "    df_clase = df[df[\"CLASE\"].str.upper() != \"SIN INFORMACI√ìN\"]\n",
        "    format_barra(df_clase[\"CLASE\"].value_counts().head(10),\n",
        "                 \"Cantidad de Accidentes por Clase\",\n",
        "                 \"N√∫mero de Accidentes\", \"Tipo de Accidente\",\n",
        "                 OUT_DIR / \"Accidentes_Clase_SVA.jpg\")\n",
        "\n",
        "    df_comuna = df[df[\"COMUNA\"].str.upper() != \"SIN INFORMACI√ìN\"]\n",
        "    format_barra(df_comuna[\"COMUNA\"].value_counts().head(10),\n",
        "                 \"Top 10 - Accidentes por Comuna\",\n",
        "                 \"N√∫mero de Accidentes\", \"Comuna\",\n",
        "                 OUT_DIR / \"Accidentes_Comuna_SVA.jpg\")\n",
        "\n",
        "    print(f\"‚úîÔ∏è  Gr√°ficas generadas en: {OUT_DIR}\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "v3GdTGq_H3Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 7: preparar_datos (preprocesamiento id√©ntico)\n",
        "def preparar_datos(data: pd.DataFrame):\n",
        "    print(\"üìÑ PREPARANDO DATOS PARA RANDOM FOREST\")\n",
        "    df_local = data.copy()\n",
        "    df_local[\"HERIDOS_MUERTOS\"] = df_local[\"GRAVEDAD_ACCIDENTE\"].str.upper().isin([\"HERIDOS\", \"MUERTOS\"]).astype(np.uint8)\n",
        "    df_local[\"FIN_DE_SEMANA\"] = df_local[\"NUM_DIA_SEMANA\"].isin([6,7]).astype(np.uint8)\n",
        "\n",
        "    numeric_features = ['NUM_MES', 'NUM_DIA_SEMANA', 'NUM_HORA', 'FIN_DE_SEMANA']\n",
        "    categorical_features = ['CLASE', 'MUNICIPIO', 'COMUNA', 'JORNADA']\n",
        "\n",
        "    # Asegurar existencia de columnas categ√≥ricas si faltan, crear con 'SIN INFORMACI√ìN'\n",
        "    for c in categorical_features:\n",
        "        if c not in df_local.columns:\n",
        "            df_local[c] = 'SIN INFORMACI√ìN'\n",
        "\n",
        "    X = df_local[numeric_features + categorical_features]\n",
        "    y = df_local['HERIDOS_MUERTOS']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y )\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_features)\n",
        "    ])\n",
        "\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    print(f\"‚úîÔ∏è  Forma despu√©s del preprocesamiento: {X_train_preprocessed.shape}\")\n",
        "    return X_train, X_test, y_train, y_test, preprocessor"
      ],
      "metadata": {
        "id": "SUJ0EmjAH7pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 8: entrenamiento, evaluaci√≥n y utilidades del modelo\n",
        "def entrenar_random_forest(X_train, X_test, y_train, y_test, preprocessor):\n",
        "    print(\"\\nü§ñ ENTRENAMIENTO RANDOM FOREST\")\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=5,      # Solo 5 √°rboles como solicitado\n",
        "            random_state=42,\n",
        "            max_depth=5          # Limitamos profundidad para evitar overfitting\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"Exactitud: {acc:.4f} | Precisi√≥n: {prec:.4f} | Sensibilidad: {rec:.4f} | F1: {f1:.4f} | AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=sns.blend_palette(PALETTE_GREEN[::-1], as_cmap=True),\n",
        "                xticklabels=['Solo Da√±os', 'Con Heridos'], yticklabels=['Solo Da√±os', 'Con Heridos'], ax=ax)\n",
        "    ax.set_title('Matriz de Confusi√≥n - Clasificaci√≥n de Accidentes', fontsize=16)\n",
        "    save_fig(fig, OUT_DIR / \"Matriz_Confusion_SVA.jpg\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.plot(fpr, tpr, color=PALETTE_GREEN[1], lw=2, label=f'ROC (AUC={roc_auc:.2f})')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\n",
        "    ax.set_xlim([-0.00, 1.01]); ax.set_ylim([-0.00, 1.01])\n",
        "    ax.set_title('Curva ROC - Clasificaci√≥n de Severidad de Accidentes')\n",
        "    ax.set_xlabel('Tasa Falsos Positivos'); ax.set_ylabel('Tasa Verdaderos Positivos')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    save_fig(fig, OUT_DIR / \"Curva_ROC_SVA.jpg\")\n",
        "\n",
        "    print(classification_report(y_test, y_pred, target_names=['Solo Da√±os', 'Con Heridos']))\n",
        "    return model, {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': roc_auc}\n",
        "\n",
        "def guardar_modelo(modelo):\n",
        "    path = MODEL_DIR / \"Modelo_RandomForest_SVA.joblib\"\n",
        "    joblib.dump(modelo, path)\n",
        "    print(f\"‚úîÔ∏è Modelo guardado en: {path}\")\n",
        "\n",
        "def hacer_predicciones(modelo):\n",
        "    nuevos = pd.DataFrame({\n",
        "        'NUM_MES': [1,7,12,3],\n",
        "        'NUM_DIA_SEMANA': [6,2,4,1],\n",
        "        'NUM_HORA': [20,14,8,18],\n",
        "        'FIN_DE_SEMANA': [1,0,0,0],\n",
        "        'CLASE': ['CHOQUE','ATROPELLO','CHOQUE','VOLCAMIENTO'],\n",
        "        'MUNICIPIO': ['MEDELL√çN']*4,\n",
        "        'COMUNA': ['LAURELES ESTADIO','LA CANDELARIA','CASTILLA','ROBLEDO'],\n",
        "        'JORNADA': ['NOCHE','TARDE','MA√ëANA','TARDE']\n",
        "    })\n",
        "    preds = modelo.predict(nuevos)\n",
        "    proba = modelo.predict_proba(nuevos)[:, 1]\n",
        "    resultados = nuevos.copy()\n",
        "    resultados['PREDICCION'] = np.where(preds==1,'CON HERIDOS','SOLO DA√ëOS')\n",
        "    resultados['PROBABILIDAD_HERIDOS'] = [f\"{p:.1%}\" for p in proba]\n",
        "    resultados['RIESGO'] = np.select([proba>0.7, proba>0.5], ['ALTO','MEDIO'], default='BAJO')\n",
        "    save_path = MODEL_DIR / \"Predicciones_Nuevos_Accidentes.csv\"\n",
        "    resultados.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úîÔ∏è Predicciones guardadas en {save_path}\")\n",
        "    return resultados\n",
        "\n",
        "def analizar_importancia_variables(modelo: Pipeline, preprocessor: ColumnTransformer):\n",
        "    rf = modelo.named_steps['classifier']\n",
        "    num = ['NUM_MES','NUM_DIA_SEMANA','NUM_HORA','FIN_DE_SEMANA']\n",
        "    cat = ['CLASE','MUNICIPIO','COMUNA','JORNADA']\n",
        "    ohe = preprocessor.named_transformers_['cat']\n",
        "    try:\n",
        "        cat_names = ohe.get_feature_names_out(cat)\n",
        "    except:\n",
        "        # compatibilidad sklearn antigua\n",
        "        cat_names = ohe.get_feature_names(cat)\n",
        "    names = num + list(cat_names)\n",
        "    imp = rf.feature_importances_\n",
        "    imp_df = pd.DataFrame({'Variable': names, 'Importancia': imp}).sort_values('Importancia', ascending=False)\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    sns.barplot(x='Importancia', y='Variable', data=imp_df.head(10), palette=paleta_antioquia(10), ax=ax)\n",
        "    ax.set_title('Top 10 Variables M√°s Importantes - Random Forest', fontsize=16)\n",
        "    save_fig(fig, MODEL_DIR / \"Importancia_Variables_RF.jpg\")\n",
        "    imp_df.to_csv(MODEL_DIR / \"Importancia_Variables_RF.csv\", index=False, encoding='utf-8-sig')\n",
        "    print(\"‚úîÔ∏è Importancia de variables guardada.\")\n",
        "    return imp_df"
      ],
      "metadata": {
        "id": "zMe4uzJoH8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 9: resumen ejecutivo y ejecuci√≥n final controlada\n",
        "def generar_resumen_final(df: pd.DataFrame, resultados: dict):\n",
        "    resumen_path = MODEL_DIR / \"Resumen_Ejecutivo_Modelo.txt\"\n",
        "    with open(resumen_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"üìÑ RESUMEN EJECUTIVO DEL PROYECTO\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        total_accidentes = len(df)\n",
        "        accidentes_con_heridos = len(df[df['GRAVEDAD_ACCIDENTE'].str.upper().isin(['HERIDOS', 'MUERTOS'])])\n",
        "        tasa_heridos = accidentes_con_heridos / total_accidentes if total_accidentes else 0\n",
        "\n",
        "        f.write(\"üìà ESTAD√çSTICAS GENERALES:\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Total de accidentes analizados: {total_accidentes:,}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Accidentes con heridos/muertos: {accidentes_con_heridos:,}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Tasa de accidentes con heridos: {tasa_heridos:.2%}\\n\\n\")\n",
        "\n",
        "        f.write(\"ü§ñ RESULTADOS DEL MODELO RANDOM FOREST:\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Exactitud: {resultados['accuracy']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Precisi√≥n: {resultados['precision']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ Sensibilidad: {resultados['recall']:.2%}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ F1-Score: {resultados['f1']:.4f}\\n\")\n",
        "        f.write(f\"   ‚Ä¢ AUC-ROC: {resultados['roc_auc']:.4f}\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            franja_peligrosa = (\n",
        "                df.groupby('JORNADA')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "            tipo_peligroso = (\n",
        "                df.groupby('CLASE')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "            comuna_peligrosa = (\n",
        "                df.groupby('COMUNA')['GRAVEDAD_ACCIDENTE']\n",
        "                .apply(lambda x: x.str.upper().isin(['HERIDOS', 'MUERTOS']).mean())\n",
        "                .idxmax() )\n",
        "\n",
        "            f.write(\"üîç HALLAZGOS PRINCIPALES:\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Franja horaria m√°s peligrosa: {franja_peligrosa}\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Tipo de accidente m√°s peligroso: {tipo_peligroso}\\n\")\n",
        "            f.write(f\"   ‚Ä¢ Comuna con mayor tasa de heridos: {comuna_peligrosa}\\n\\n\")\n",
        "\n",
        "            f.write(\"üí° RECOMENDACIONES:\\n\")\n",
        "            f.write(f\"   1. Reforzar vigilancia en: {franja_peligrosa}\\n\")\n",
        "            f.write(f\"   2. Implementar campa√±as preventivas para: {tipo_peligroso}\\n\")\n",
        "            f.write(f\"   3. Focalizar recursos de control en: {comuna_peligrosa}\\n\")\n",
        "            f.write(\"   4. Utilizar el modelo predictivo para priorizar zonas de riesgo.\\n\")\n",
        "        except Exception as e:\n",
        "            f.write(f\"‚ùå No se pudieron generar hallazgos detallados: {e}\\n\")\n",
        "\n",
        "    print(f\"‚úîÔ∏è Resumen ejecutivo guardado en: {resumen_path}\")\n",
        "\n",
        "# EJECUCI√ìN CONTROLADA (main)\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df_db = load_table(DB_PATH, TABLE_NAME)\n",
        "        analisis_rapido(df_db)\n",
        "        X_train, X_test, y_train, y_test, preprocessor = preparar_datos(df_db)\n",
        "        modelo_rf, resultados = entrenar_random_forest(X_train, X_test, y_train, y_test, preprocessor)\n",
        "        preds_df = hacer_predicciones(modelo_rf)\n",
        "        guardar_modelo(modelo_rf)\n",
        "        imp_df = analizar_importancia_variables(modelo_rf, preprocessor)\n",
        "        generar_resumen_final(df_db, resultados)\n",
        "        print(\"\\n‚úîÔ∏è Flujo completo: ejecutado correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la ejecuci√≥n: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMfkTi8GIAA6",
        "outputId": "a397e5fa-8e5b-4a8a-8592-313e1ffd18ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ AN√ÅLISIS EXPLORATORIO R√ÅPIDO\n",
            "‚úîÔ∏è  Gr√°ficas generadas en: /content/Graficas_Salida\n",
            "============================================================\n",
            "üìÑ PREPARANDO DATOS PARA RANDOM FOREST\n",
            "‚úîÔ∏è  Forma despu√©s del preprocesamiento: (162748, 47)\n",
            "\n",
            "ü§ñ ENTRENAMIENTO RANDOM FOREST\n",
            "Exactitud: 0.7679 | Precisi√≥n: 0.9501 | Sensibilidad: 0.5693 | F1: 0.7120 | AUC-ROC: 0.8274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Solo Da√±os       0.69      0.97      0.81     20187\n",
            " Con Heridos       0.95      0.57      0.71     20500\n",
            "\n",
            "    accuracy                           0.77     40687\n",
            "   macro avg       0.82      0.77      0.76     40687\n",
            "weighted avg       0.82      0.77      0.76     40687\n",
            "\n",
            "‚úîÔ∏è Predicciones guardadas en /content/Modelo_Predict/Predicciones_Nuevos_Accidentes.csv\n",
            "‚úîÔ∏è Modelo guardado en: /content/Modelo_Predict/Modelo_RandomForest_SVA.joblib\n",
            "‚úîÔ∏è Importancia de variables guardada.\n",
            "‚úîÔ∏è Resumen ejecutivo guardado en: /content/Modelo_Predict/Resumen_Ejecutivo_Modelo.txt\n",
            "\n",
            "‚úîÔ∏è Flujo completo: ejecutado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELDA 10: listar archivos clave generados\n",
        "import glob\n",
        "print(\"\\nArchivos generados (jpg, db, csv, joblib, txt):\")\n",
        "for file in sorted(glob.glob(\"/content/**/*\", recursive=True)):\n",
        "    if any(file.lower().endswith(ext) for ext in [\".jpg\", \".db\", \".csv\", \".joblib\", \".txt\"]):\n",
        "        print(\" \", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4YqIrZFIDII",
        "outputId": "b864fbfb-a4e3-4a73-d180-3935f6c69498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivos generados (jpg, db, csv, joblib, txt):\n",
            "  /content/Graficas_Salida/Accidentes_Clase_SVA.jpg\n",
            "  /content/Graficas_Salida/Accidentes_Comuna_SVA.jpg\n",
            "  /content/Graficas_Salida/Accidentes_Gravedad_SVA.jpg\n",
            "  /content/Graficas_Salida/Accidentes_Jornada_SVA.jpg\n",
            "  /content/Graficas_Salida/Curva_ROC_SVA.jpg\n",
            "  /content/Graficas_Salida/Matriz_Confusion_SVA.jpg\n",
            "  /content/Modelo_Predict/Importancia_Variables_RF.csv\n",
            "  /content/Modelo_Predict/Importancia_Variables_RF.jpg\n",
            "  /content/Modelo_Predict/Modelo_RandomForest_SVA.joblib\n",
            "  /content/Modelo_Predict/Predicciones_Nuevos_Accidentes.csv\n",
            "  /content/Modelo_Predict/Resumen_Ejecutivo_Modelo.txt\n",
            "  /content/Proyecto_Accidentalidad_Vial_Antioquia.db\n"
          ]
        }
      ]
    }
  ]
}